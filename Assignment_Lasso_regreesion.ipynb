{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiqVDHpdSoaYB7DtFJeIWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Regression/blob/main/Assignment_Lasso_regreesion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa0paWxuEtM4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Lasso Regression, and how does it differ from other regression techniques?**"
      ],
      "metadata": {
        "id": "jEeRqEXzHIyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean.\n",
        "- “LASSO” stands for Least Absolute Shrinkage and Selection Operator. It is a statistical formula for the regularisation of data models and feature selection.\n",
        "- Lasso Regression uses L1 regularization technique, Regularization is an important concept that is used to avoid overfitting of the data, especially when the trained and test data are much varying.\n",
        "-  Lasso Regression. regularization techniques differ in the way they assign a penalty to the coefficients. "
      ],
      "metadata": {
        "id": "eXZqzUCsILyv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Iot5HCDHKWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the main advantage of using Lasso Regression in feature selection?**"
      ],
      "metadata": {
        "id": "67sH1ZzBHKpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automatic features selection**. The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own.\n",
        "\n",
        "**Reduced overfitting** Another advantage of a LASSO regression is that the L1 penalty that is added to the model helps to prevent the model from overfitting. This makes intuitive sense because when the model sets feature coefficients to zero and effectively removes features from the model, model complexity decreases."
      ],
      "metadata": {
        "id": "rEwczLRZJb8l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3rTcFUM6HPcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. How do you interpret the coefficients of a Lasso Regression model?**"
      ],
      "metadata": {
        "id": "8FiyS4hzHPtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Lasso regression, the regularization penalty can lead to some of the coefficients being shrunk to zero, resulting in a sparse model. The number of non-zero coefficients can be used to evaluate the effectiveness of the regularization and feature selection."
      ],
      "metadata": {
        "id": "pmFdFKfAM7ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6e6yllkQHVou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
        "model's performance?**"
      ],
      "metadata": {
        "id": "py7kiTHTHV1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean. Shrinkage results in simple, sparse models which are easier to analyze than high-dimensional data models with large numbers of parameters.\n",
        "\n",
        "- When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
        "- As λ increases, more and more coefficients are set to zero and eliminated.\n",
        "- When λ = ∞, all coefficients are eliminated."
      ],
      "metadata": {
        "id": "6R9Dr4V9OJCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The penalty term is the sum of the absolute values of the coefficients (L1 regularization)."
      ],
      "metadata": {
        "id": "k4baLO80Ofhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a New Train and Validation Datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_train, data_val = train_test_split(new_data_train, test_size = 0.2, random_state = 2)"
      ],
      "metadata": {
        "id": "HzwgiybbHaW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifying Predictors and Target\n",
        "#Classifying Independent and Dependent Features\n",
        "#_______________________________________________\n",
        "#Dependent Variable\n",
        "Y_train = data_train.iloc[:, -1].values\n",
        "#Independent Variables\n",
        "X_train = data_train.iloc[:,0 : -1].values\n",
        "#Independent Variables for Test Set\n",
        "X_test = data_val.iloc[:,0 : -1].values"
      ],
      "metadata": {
        "id": "YQ83KUvMHbO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating The Model With RMLSE\n",
        "def score(y_pred, y_true):\n",
        "error = np.square(np.log10(y_pred +1) - np.log10(y_true +1)).mean() ** 0.5\n",
        "score = 1 - error\n",
        "return score\n",
        "actual_cost = list(data_val['COST'])\n",
        "actual_cost = np.asarray(actual_cost)"
      ],
      "metadata": {
        "id": "9eEy7wWiOpCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Lasso Regressor\n",
        "#Lasso Regression\n",
        "\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "#Initializing the Lasso Regressor with Normalization Factor as True\n",
        "lasso_reg = Lasso(normalize=True)\n",
        "#Fitting the Training data to the Lasso regressor\n",
        "lasso_reg.fit(X_train,Y_train)\n",
        "#Predicting for X_test\n",
        "y_pred_lass =lasso_reg.predict(X_test)\n",
        "#Printing the Score with RMLSE\n",
        "print(\"\\n\\nLasso SCORE : \", score(y_pred_lass, actual_cost))\n"
      ],
      "metadata": {
        "id": "G20BOFzfOpFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ByRRvhYPA60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?**"
      ],
      "metadata": {
        "id": "bpzN72yHHbZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- yes, Lasso Regression be used for non-linear regression problems.\n",
        "\n",
        "Regularization with a lasso penalty is an advantageous in that it estimates some coefficients in linear regression models to be exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby selecting the number of basis functions effectively. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gjdAGT03QeGs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImEezBoTHePl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jw3CwrSmHe3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. What is the difference between Ridge Regression and Lasso Regression?**"
      ],
      "metadata": {
        "id": "IwRr8RsRHe_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Regression**"
      ],
      "metadata": {
        "id": "HnfETkqyQ9ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- while a model using L2 is called Ridge Regression. \n",
        "- The penalty term is the sum of the squares of the coefficients (L2 regularization).\n",
        "- Shrinks the coefficients but doesn’t set any coefficient to zero.\n",
        "- Helps to reduce overfitting by shrinking large coefficients.\n",
        "- Works well when there are a large number of features.\n",
        "- Performs “soft thresholding” of coefficients."
      ],
      "metadata": {
        "id": "ge3jrM0dQ7jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso Regression**\n",
        "- A regression model using the L1 regularization technique is called Lasso Regression, \n",
        "- The penalty term is the sum of the absolute values of the coefficients (L1 regularization).\n",
        "- Can shrink some coefficients to zero, effectively performing feature selection.\n",
        "- Helps to reduce overfitting by shrinking and selecting features with less importance\n",
        "- Works well when there are a small number of features.\n",
        "- Performs “hard thresholding” of coefficients."
      ],
      "metadata": {
        "id": "mqOi41DuRO_s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bUyeobmmHiYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "At1c9DR_HjJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?**"
      ],
      "metadata": {
        "id": "zZ3LwFQAHjRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated this can inflate our regression coefficients.\n",
        "- To reduce multicollinearity we can use regularization that means to keep all the features but reducing the magnitude of the coefficients of the model. This is a good solution when each predictor contributes to predict the dependent variable.\n",
        "- LASSO when we increase the value of LAMBDA the most important parameters shrink a little bit and the less important parameters goes closed to ZERO.\n",
        "So, LASSO is able to exclude silly parameters from the model."
      ],
      "metadata": {
        "id": "qWoqQnmMBYqt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cgPbH6-MHmr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRoDI1cgHnNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?**"
      ],
      "metadata": {
        "id": "TPoi57C1Hnbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fit the lasso regression model and choose a value for λ**\n",
        "\n",
        "lasso regression is appropriate to use, we can fit the model (using popular programming languages like R or Python) using the optimal value for λ.\n",
        "\n",
        "To determine the optimal value for λ, we can fit several models using different values for λ and choose λ to be the value that produces the lowest test MSE.\n",
        "\n",
        "- LASSO when we increase the value of LAMBDA the most important parameters shrink a little bit and the less important parameters goes closed to ZERO. So, LASSO is able to exclude silly parameters from the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pim5aF6nD5o1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XXTpAeCHqeM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
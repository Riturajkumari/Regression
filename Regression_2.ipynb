{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyDvj9NtarxO1FXCBC4SES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Regression/blob/main/Regression_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
        "represent?**"
      ],
      "metadata": {
        "id": "omGJUJljubf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- R-squared is a goodness-of-fit measure for linear regression models. It indicates the percentage of the variance in the dependent variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale. \n",
        "- The R-squared value ranges from 0 to 1. An R-squared value of 1 indicates that the model perfectly fits the data and there is no difference between the predicted value and actual value.\n",
        "- The formula for calculating R-squared is:\n",
        "\n",
        "      R-squared = 1 - (SSres / SStot)\n",
        "\n",
        "       where SSres is the sum of squared residuals and SStot is the total sum of squares."
      ],
      "metadata": {
        "id": "ZANjmFwj7pyD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TV1qNBEnugX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.**"
      ],
      "metadata": {
        "id": "NoHpCQLuuglJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Adjusted R-squared is a statistical tool that measures the accuracy of a regression model. It is a modified version of R-squared that accounts for predictors that are not significant in a regression model. \n",
        "- the adjusted R-squared shows whether adding additional predictors improve a regression model or not. Compared to a model with additional input variables, a lower adjusted R-squared indicates that the additional input variables are not adding value to the model."
      ],
      "metadata": {
        "id": "4kZbT7wV9ODj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a regression model.\n",
        "\n",
        "- It is calculated as:\n",
        "\n",
        "- Adjusted R2 = 1 – [(1-R2)*(n-1)/(n-k-1)]\n",
        "\n",
        "where:\n",
        "\n",
        "   - R2: The R2 of the model\n",
        "   - n: The number of observations\n",
        "   - k: The number of predictor variables"
      ],
      "metadata": {
        "id": "4EFcqB1D82rD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yp2iVfhWukS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. When is it more appropriate to use adjusted R-squared?**"
      ],
      "metadata": {
        "id": "vjtF9PgZukhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. It determines the extent of the variance of the dependent variable, which the independent variable.\n",
        "- The adjusted R-squared increases when a new term improves the model more than would be expected by chance, and decreases when a predictor improves the model by less than expected.\n",
        "- Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a regression model."
      ],
      "metadata": {
        "id": "xK3fcBY69n2g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Sy6WQaXunuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
        "calculated, and what do they represent?**"
      ],
      "metadata": {
        "id": "e2ArZvsLuoNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The MSE, MAE, RMSE, and R-Squared metrics are mainly used to evaluate the prediction error rates and model performance in regression analysis.\n",
        "-  In regression analysis, MAE (Mean Absolute Error), MSE (Mean Squared Error), and RMSE (Root Mean Squared Error) are used to evaluate the performance of a regression model.\n",
        "The MSE, MAE, RMSE, and R-Squared metrics are mainly used to evaluate the prediction error rates and model performance in regression analysis.\n",
        "  \n",
        "- MAE (Mean absolute error) represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
        "- MSE (Mean Squared Error) represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
        "- RMSE (Root Mean Squared Error) is the error rate by the square root of MSE.\n",
        "- R-squared (Coefficient of determination) represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is."
      ],
      "metadata": {
        "id": "EtUKB_UJ-p2H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XpOy9HtwusgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
        "regression analysis.**"
      ],
      "metadata": {
        "id": "4IYqqB3Gus3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) are all commonly used evaluation metrics in regression analysis.\n",
        "\n",
        "- The MAE is the average of the absolute differences between predicted and actual values. It is less sensitive to outliers than MSE and RMSE.\n",
        "\n",
        "- The MSE is the average of the squared differences between predicted and actual values. It is more sensitive to outliers than MAE but less sensitive than RMSE.\n",
        "\n",
        "- The RMSE is the square root of the average of the squared differences between predicted and actual values. It is more sensitive to outliers than both MAE and MSE."
      ],
      "metadata": {
        "id": "mP4rYeFlA8l_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqhXUXc3uyzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
        "it more appropriate to use?**"
      ],
      "metadata": {
        "id": "tR3hTiuouzY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- if you have many features with high correlation and you need to take away the useless features then LASSO is the better solution. If the number of features greater than the number of observations and many features with multi-collinearity, Ridge regularization is a better solution.\n",
        "**LASSO stands for Least Absolute Shrinkable and Selection Operator**. As mentioned in the regularization definition, it is the process of adding information to prevent the over-fitting problem.\n",
        "\n",
        "- Lasso and Ridge regularization are two types of regularization techniques used in linear regression to prevent overfitting. Both techniques add a penalty term to the cost function of the linear regression model. The difference between Lasso and Ridge regularization is that Lasso adds an L1 penalty term while Ridge adds an L2 penalty term"
      ],
      "metadata": {
        "id": "1hvCdNrYBThS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DtBtLSa1u4KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
        "example to illustrate.**"
      ],
      "metadata": {
        "id": "rr-QH9XWu4Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the loss function that the model is trying to minimize. This penalty term is proportional to the magnitude of the coefficients of the model. By doing so, the model is encouraged to keep the coefficients small and thus avoid overfitting.  Regularization can be applied to any linear model, including linear regression and logistic regression.\n"
      ],
      "metadata": {
        "id": "HGLMhajLCYOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Regularization means restricting a model to avoid overfitting by shrinking the coefficient estimates to zero. When a model suffers from overfitting, we should control the model's complexity. Technically, regularization avoids overfitting by adding a penalty to the model's loss function:\n",
        "      -  Regularization= Loss function+Penalty\n",
        "\n",
        "There are three commonly used regularization techniques to control the complexity of machine learning models, as follows:\n",
        "\n",
        "- L2 regularization\n",
        "- L1 regularization\n",
        "- Elastic Net"
      ],
      "metadata": {
        "id": "QomvAAakCyQE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UQ1Z6Kzu8DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
        "choice for regression analysis.**"
      ],
      "metadata": {
        "id": "QI1YaREHu8kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Regularized linear models are a class of linear models that use regularization methods to constrain or shrink the coefficient estimates towards zero. These models are used to prevent overfitting and improve the generalization of the model. However, there are some limitations to regularized linear models that make them not always the best choice for regression analysis.\n"
      ],
      "metadata": {
        "id": "7ME3gPerDVz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "."
      ],
      "metadata": {
        "id": "MMsozTJhvA_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
        "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
        "performer, and why? Are there any limitations to your choice of metric?**"
      ],
      "metadata": {
        "id": "HmSahXcOvBNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Both RMSE and MAE are used to evaluate the accuracy of regression models. RMSE is more sensitive to outliers and penalizes large errors more than MAE because errors are squared initially.\n",
        "- On the other hand, MAE represents the average error when making a prediction with the model and doesn’t have a predilection for small or big errors"
      ],
      "metadata": {
        "id": "priyzeWzD1-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- n your case, Model A has an RMSE of 10 while Model B has an MAE of 8. Since both models have different evaluation metrics, it is not possible to compare them directly. However, if we consider that both models have similar performance, then we can say that Model B is better than Model A because it has a lower error rate.RMSE is more sensitive to outliers than MAE. "
      ],
      "metadata": {
        "id": "ELSTrhtSEcH9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEhMJ1BovEZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1qMhg2NvFLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. You are comparing the performance of two regularized linear models using different types of\n",
        "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
        "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
        "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
        "method?**"
      ],
      "metadata": {
        "id": "Wo-rsER-vFbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Regularization came out to be an essential technique for reducing the effect of overfitting, especially for regression problems. An overfitting model has a large variation in Train set Root Mean Square Error (RMSE) and test Root Mean Square Error (RMSE). Regularized Regression Model tends to show the least difference between the Train and Test Set RMSE than the Classical Regression Model."
      ],
      "metadata": {
        "id": "t1sKJXtuEyqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Regularization in Linear regression is a technique that prevents overfitting in the model by penalizing the coefficients involved in the linear regression equation. Coefficients in an overfitted model are inflated or weigh highly. Thus adding penalties on these parameters prevent them from inflating. Overfitted Models perform well on the training data while fail to perform on the test or new data passed. "
      ],
      "metadata": {
        "id": "KaPyN_KJE7ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Two Regularization techniques can be used to present overfitting. The L1 Regularization or LASSO adds the absolute value of coefficients as penalties to the cost function. The L2 Regularization or Ridge adds the summation of squared values of coefficients as penalities to the cost function. The alpha value represents how much we want to penalize the coefficients."
      ],
      "metadata": {
        "id": "JZurjsa2Fa6X"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvBtj3ScdV7C5Kx+dwx3zW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Regression/blob/main/Assignment_Ridge_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMmuS59V3ifw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?**"
      ],
      "metadata": {
        "id": "WFTPA7gG3lUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge regression** is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values. "
      ],
      "metadata": {
        "id": "0jC0k1dn8HS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*differ from ordinary least squares regression*\n",
        "- Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS estimator.\n",
        "- Ridge Regression added a term in ordinary least square error function that regularizes the value of coefficients of variables. This term is the sum of squares of coefficient multiplied by the parameter The motive of adding this term is to penalize the variable corresponding to that coefficient not very much correlated to the target variable. This term is called L2 regularization. "
      ],
      "metadata": {
        "id": "sFZN3fv66_Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LB32C0qD4Ezk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3HQyFnY4E2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the assumptions of Ridge Regression?**"
      ],
      "metadata": {
        "id": "8iL7741J4E_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed."
      ],
      "metadata": {
        "id": "8fmnQwHv8iUi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZglKs9KM4Jxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-YCTL9F4KeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?**"
      ],
      "metadata": {
        "id": "O33KrnJM4KoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean. Shrinkage results in simple, sparse models which are easier to analyze than high-dimensional data models with large numbers of parameters.\n",
        "\n",
        "- When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
        "- As λ increases, more and more coefficients are set to zero and eliminated.\n",
        "- When λ = ∞, all coefficients are eliminated."
      ],
      "metadata": {
        "id": "IZOlL3iW9TR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Choose the tuning parameter of a ridge regression using cross-validation.\n",
        "- Usage\n",
        " ne.lambda.cv(y, x, lambda, fold) \n",
        "- Arguments\n",
        "y\t\n",
        "Length n response vector.\n",
        "\n",
        "x\t\n",
        "n x p matrix for covariates with p variables and n sample size.\n",
        "\n",
        "lambda\t\n",
        "A numeric vector for candidate tuning parameters for a ridge regression.\n",
        "\n",
        "fold\t\n",
        "fold-cross validation used to choose the tuning parameter.\n",
        "- Value\n",
        "lambda\t\n",
        "The selected tuning parameter, which minimizes the prediction error.\n",
        "\n",
        "spe\t\n",
        "The prediction error for all of the candidate lambda values."
      ],
      "metadata": {
        "id": "22rY9fDC9o8a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGjCYTcp4OeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9vMoGqX4PQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Can Ridge Regression be used for feature selection? If yes, how?**"
      ],
      "metadata": {
        "id": "bzPcvlC74Pab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use ridge regression for feature selection while fitting the model. In this article, we are going to use logistic regression for model fitting and push the parameter penalty as L2 which basically means the penalty we use in ridge regression. "
      ],
      "metadata": {
        "id": "056MlRdx-xxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ud4-rzJE4Sij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_logit =LogisticRegression(C=1, penalty='l2')\n",
        "ridge_logit.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "4jJ-xAm24TLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the coefficient of features that will tell us how the features are important for the model.\n",
        "ridge_logit.coef_ \n",
        "#  feature from the data is one of the most important features and other features are not that much important."
      ],
      "metadata": {
        "id": "XhdlYX1M4TSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how many features are having a coefficient of more than zero \n",
        "\n",
        "np.sum(ridge_logit.coef_ >= 0)"
      ],
      "metadata": {
        "id": "oqYDETkV_Fbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqGvi5SJ_lnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. How does the Ridge Regression model perform in the presence of multicollinearity?**"
      ],
      "metadata": {
        "id": "VIFXDXHZ4Tds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.\n",
        "When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they\n",
        "may be far from the true value.\n",
        "- Multicollinearity, or collinearity, is the existence of near-linear relationships among the independent variables.\n",
        "- To reduce multicollinearity we can use regularization that means to keep all the features but reducing the magnitude of the coefficients of the model. This is a good solution when each predictor contributes to predict the dependent variable."
      ],
      "metadata": {
        "id": "3jclng9FABM5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_DXl_6E_sdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-T2VOn634XoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2255IjUZ4YZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Can Ridge Regression handle both categorical and continuous independent variables?**"
      ],
      "metadata": {
        "id": "elEY6asZ4Yhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is used for regression purpose only as it needs the dependent variable to be continuous.\n",
        "\n",
        "So for your analysis Ridge regression can't be used.\n",
        "\n",
        "Special characteristic of Ridge regression is it works fine in presence of multicollinearity but with a continuous dependent variable."
      ],
      "metadata": {
        "id": "Y4Jhw42JBMex"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BrwO2W1u4gKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixGCIv3i4gM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. How do you interpret the coefficients of Ridge Regression?**"
      ],
      "metadata": {
        "id": "MBfhNFFL4ggU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ridge coefficients are a reduced factor of the simple linear regression coefficients and thus never attain zero values but very small values. \n",
        "- Ridge: It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity.\n",
        "- Ridge: It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in the case of exorbitantly high #features, say in millions, as it will pose computational challenges.\n",
        "- Ridge: It generally works well even in the presence of highly correlated features, as it will include all of them in the model. Still, the coefficients will be distributed among them depending on the correlation."
      ],
      "metadata": {
        "id": "nDuQ7Fs6BotB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRk-XdRN4jx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xzFK7l74ksU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?**"
      ],
      "metadata": {
        "id": "dEwbsO5j4pQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - The ridge regression technique can be used to predict time-series.\n",
        " - Time series regression can help you understand and predict the behavior of dynamic systems from experimental or observational data.\n",
        "\n",
        " ridge regression as an approach for forecasting with many predictors that are related to the target variable. Ridge regression is an expansion of linear regression. It’s fundamentally a regularization of the linear regression model. Ridge regression uses the damping factor (λ) as a scalar that should be learned, normally it will utilize a method called cross-validation to find the value. But in  research, we will calculate the damping factor/ridge regression in the ridge regression (RR) model firsthand to minimize the running time used when using cross-validation. The RR model will be used to for example : **forecast the food price time-series data**. The proposed method shows that calculating the damping factor/regression estimator first results in a faster computation time compared to the regular RR model."
      ],
      "metadata": {
        "id": "1rDzVDtyDAcR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYUbpIpD4q2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}